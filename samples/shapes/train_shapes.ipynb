{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "set_session(sess)\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM4UlEQVR4nO3df6xedX0H8PfntqW0tLSUYgtFiu0QATcgOBBxClrkhzKchUSHMp1GzASzabJsccncdFt0/rFkTrMlzm2ZyZZshr/YXJw/Jk6mYSYThCoDfxRRRGop9Mel7Xd/3NPt7nrbe2773Pv03vt6JU/6nO9z+v1+znPPH+d9vuecp1prAQAA6GNk2AUAAABzhwABAAD0JkAAAAC9CRAAAEBvAgQAANCbAAEAAPQmQAAAAL0dU4CoqrOr6jMT2h46in7uqqqLu/fXV9WOqqpu+UNV9aYefby/qr4zvp6quriqvlRV/1ZVn62qTV37pq7t81X1uao68wj9bq6qe6vq6ap66bj2P6mqe7rXb41r/+2q+mpVfaWq3j2N7+CvDvVfVXu6ur5cVZ+uqssnWf/NVfVIVb29W/7jqvpCN+4fd23Lum2c9t8EAAAmc7zMQNyd5Iru/RVJ7k1ywbjlL/bo46NJrprQ9liSa1trL0vy4SS/17X/WpKPt9auTPLXSe44Qr+PJbk6yT9MaP+z1tqLk7wkyY1d0FiZ5FeTHGp/R1Wd1KP2iR5trV3VWrs8ybuS/GVVrZ1kvY+31v6ie//e1trLW2uXJrm0qi5ore3pthEAAAZiVgJEVX20qm6tqpHujPplE1a5O8mhs/sXJvlYkpdW1dIk61pr355qjNbaY0kOTmj7QWttV7e4L8n+7v39SVZ3709J8nhVLa2qu6vqBVW1vjuTf0prbXdr7clJxvtW9+/Brt8DSfYk+X6SZd1rT5Jnp6p9iu3aluRTSa5Jkqr65GHWG+0+X5Lk6a4OAAAYqMUD6OOSqvr8FOu8O8lnMzab8K+ttf+Y8PlXMnaWfUmSlrEZhw8nuS/JV5Oku4znjybp+/dba5890uDdLMAHkry1a/pMkk9X1VuTLE1yaWttX7f8iSQ7k/x6a23HFNuVqrolycOHQk5V3ZVkW8bC2QcOHdgfo+8l2ZAkrbVbjlDLnyZ5bZJ/ydg2MAdV1e1JbkryUGvtbcOuh4XJfsjxwH7IsNkHJzeIAHFva23LoYXJrrdvre2tqk8k+VCS0w/z+eNJXpfka621x6tqfcZmJe7u1vlykiunW1wXSv4+yQdba9/omj+Y5Hdaa5+qqjck+cMk72ytbauqR5Ksaa39e4++tyR5S5IbuuXnJ9maZFPGAsQXqurO1tqj0617gucm+cZUK7XW7qiq30jyj0muTXLXMY7LELTWPpLkI8Oug4XNfsjxwH7IsNkHJzdblzCdnrGz/+/P2MH6ZO5O8ptJvtQtfz/Jzenuf6iqy7sbgie+XnGEcUeS/G2SO1trd47/KMkT3fvHk6zp1r86yZIkT1TVL06xTZd123NTa23PuH53tdb2dW37kqw4Uj9TqapzkvxSkk9Psd6JSdJa25/kmSS7j2VcAACYzCBmII6oO4j/RMYuCbqnqv6uqq5vrU08O353kvckuadb/lKSGzN2GdOUMxDdFNPrk5zXPYnptiQXJ3l1knVV9cYkX2+t3ZGxy5n+vKr2Zyww3FZVz0nyBxm712B/ks9U1X8meSpj9yCcn+SCqrqrtfa7ST7eDX1n98Co97TW7u3unbgnY2Hic909DNO1oao+l7HLq55J8vbW2hPddn7yMJcxfbKqTu2254uttc8fxbgAAHBE1Vobdg0chaq6Kcl7k3xs3JOYJq6zLMk/JYmnMQEAMAgCBAAA0Nvx8jsQAADAHCBAAAAAvR3xJuo733em65sWkNe+b3sNu4bJLLv4dvvhArLnax857vZD++DCcjzug4n9cKGxH3I8ONx+aAYCAADoTYAAAAB6EyAAAIDeBAgAAKA3AQIAAOhNgAAAAHoTIAAAgN4ECAAAoDcBAgAA6E2AAAAAehMgAACA3gQIAACgNwECAADoTYAAAAB6EyAAAIDeBAgAAKA3AQIAAOhNgAAAAHoTIAAAgN4ECAAAoDcBAgAA6E2AAAAAehMgAACA3gQIAACgNwECAADoTYAAAAB6EyAAAIDeBAgAAKA3AQIAAOhNgAAAAHoTIAAAgN4EiCFoVWnDLgIAAI6CADHLDi4aydfv2JrRVScJEQAAzDkCxCy7/7YbM7pqRb7+rptz4MQThl0OAABMiwAxi/YvXZJU/d/ysqVmIQAAmFMEiFm07dZrs2/Nyf+7fN/tW3NwyeIhVgQAANMjQMyS0RXL0hYt+un21SvMQgAAMGcIELNg36qT8q03bMne01b/1Gf3v+O1aYv8GQAAmBscuc6Cb7/miuxZf+phP999+qlmIQAAmBMEiBm2Z+2qKZ+29OBbXp2nN64TIgAAOO4JEDNs+ysuye4z1k653rZbr5uFagAA4NgIEDPomTPW5tmVy3uvv+O8jWYhAAA4rgkQM+iHl57Xa/bhkIe3XjlzxQAAwAAIEDNk11nrsnfcbz709aNLzp2BagAAYDAEiBnw1Nnrs33Li7J7w2nT+49V+e51L56ZogAAYAAEiBmw85zn5pnphodxHn3ZRYMrBgAABkiAGLCdmzdk11nrjr6Dqjz2sguz/RWXDK4oAAAYEAFiwJ4+8znTunF6UlX54WXnD6YgAAAYIAFigH5yzpnZ8YKzBtJXG6k88porBtIXAAAMigAxIDs3b8j2V74oe59zymA6HBnJjvM3DqYvAAAYEAFiQPatXpG9p60eaJ8HlyzOQzddOdA+AQDgWAgQA7Bz84b84PIXDr7jkZHs2rh+8P0CAMBREiAG4NnlJ2b0lJUz0veBE0/ItlteNSN9AwDAdAkQx2jnpjOyfcsMPnJ1ZCS7zl6fbW+8ZubGAACAngSIY3Rg6ZLsX7F8ZgcZGcm+1StmdgwAAOhBgDgGT21cn+/M0qNWR1edlAdvvXZWxgIAgMNZPOwC5qqnN5yWh17/yhw8YcnsDDgykv3Ll87OWAAAcBhmII5SG6nZCw+dvWtX58FfuW5WxwQAgPEEiKPwzPpT8803DeFyoqocXLxo9scFAICOAHE0KmmLhvPV7T79VPdCAAAwNALENLQke9auzgNvfc3wiqhKqtKGVwEAAAuYADENo6tX5P533Dh2ED9ET5+1Lt/65auFCAAAZp0A0VNL0rqz/8eDpzZvyMNbrxx2GQAALDACRE/PrliW+27fOuwy/p82Ujk44k8IAMDscfTZQ0tm/ZGtffzkBRvznRteMuwyAABYQASIKbQk+9aszH3vfN2wS5nUwUWLcmCJ3wMEAGB2CBBTOHjCktz3zuPr0qXxdlzwvGzf8qJhlwEAwAIhQBxBS7Jv9YphlzGlA0uXZP+yE4ZdBgAAC4AAcQRt0Ui+cduNwy5jSk/+7OZ8b8vP59nlS4ddCgAA85wAcRgtY7/6PFf8+KJz8qNLzh12GQAAzHMCxOGMVB58y6uHXcW0jK5akdGVy4ddBgAA85gAMYmWZOfmDcMuY9qeuPj5efL8s4ddBgAA85gAMUFLsuO8jXno9VuGXcpR2bNuzZy48RsAgLlJgJjEw1uvHHYJR+3HF/5Mntp0xrDLAABgnhIgJpgPNyLv2rg+e9ecPOwyAACYhwSICb573YuTqmGXcUyefOGmPPryi7L3lJXDLgUAgHlGgBjn0ZdfNOwSBmbHCzdl72mrh10GAADzjADR2f7KS/LYL1w452cfxnvionNcygQAwEAJEJ0fXnr+vAoPSfKTc8/K6KqThl0GAADziACR5JEbrkgbmZ9fxWNX/JxZCAAABmZ+HjVP047zz05G5tfswyG7nnd69i9fOuwyAACYJxZ8gHjo5qtycPGiYZcxo757zWWeyAQAwEAs+ACxa+P6ZJ5evnTI7jPW5r9vvir7TnY/BAAAx2Z+HzlPYdsbX5UDS5cMu4xZsWfdmrR5PtMCAMDMW7ABYtubrlkQsw/jffOWqzO6YtmwywAAYA5bOEfPE+xbvXJBhYckGV29Mm3RwtpmAAAGa0EeTT7w5uszevLyYZcxFA+87YY8u/zEYZcBAMActSADxP7lSxfc7MMh+5efmMzPJ9YCADALFtxR9ANvvj77FvgPq/3XHTdl/9IThl0GAABz0OJhFzDbzv2bf3YGPkkdODjsEgAAmIMWXIAYOejAGQAAjtaCu4QJAAA4egIEAADQmwABAAD0JkAAAAC9CRAAAEBvAgQAANCbAAEAAPQmQAAAAL0JEAAAQG8CBAAA0JsAAQAA9CZAAAAAvQkQAABAbwIEAADQmwABAAD0JkAAAAC9CRAAAEBvAgQAANCbAAEAAPQmQAAAAL0JEAAAQG8CBAAA0JsAAQAA9CZAAAAAvQkQAABAbwIEAADQmwABAAD0JkAAAAC9CRAAAEBvAgQAANCbAAEAAPQmQAAAAL1Va23YNQAAAHOEGQgAAKA3AQIAAOhNgAAAAHoTIAAAgN4ECAAAoDcBAgAA6O1/AH2UttxP9SGfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "global sess\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "\n",
    "    # Create model in training mode\n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                              model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global sess\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "    \n",
    "    # Which weights to start with?\n",
    "    init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "    if init_with == \"imagenet\":\n",
    "        model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "    elif init_with == \"coco\":\n",
    "        # Load weights trained on MS COCO, but skip layers that\n",
    "        # are different due to the different number of classes\n",
    "        # See README for instructions to download the COCO weights\n",
    "        model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                           exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                    \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    elif init_with == \"last\":\n",
    "        # Load the last model you trained and continue training\n",
    "        model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: D:\\NovarumDX\\repos\\Mask_RCNN\\logs\\shapes20220803T1854\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_2:0\", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_1:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_5:0\", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_2:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_8:0\", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/sub_3:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/GatherV2_11:0\", shape=(?, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_2:0\", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_1:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_5:0\", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_2:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_8:0\", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/sub_3:0\", shape=(?,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/GatherV2_11:0\", shape=(?, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_8_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_9_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_10_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_11_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_12_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_13_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_14_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mel\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape_1:0\", shape=(4092,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Reshape:0\", shape=(4092, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI/GatherV2_15_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Could not find variable training/SGD/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training/SGD/learning_rate/class tensorflow::Var does not exist.\n\t [[{{node training/SGD/Identity/ReadVariableOp}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23376\\4202044036.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# layers. You can also pass a regular expression to select\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# which layers to train by name pattern.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     model.train(dataset_train, dataset_val, \n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\NovarumDX\\repos\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2379\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2381\u001b[1;33m         self.keras_model.fit(\n\u001b[0m\u001b[0;32m   2382\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2383\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    779\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\keras\\engine\\training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    568\u001b[0m     training_utils_v1.check_generator_arguments(\n\u001b[0;32m    569\u001b[0m         y, sample_weight, validation_split=validation_split)\n\u001b[1;32m--> 570\u001b[1;33m     return fit_generator(\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\keras\\engine\\training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1075\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1077\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4019\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   4020\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   4021\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\mask-rcnn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Could not find variable training/SGD/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training/SGD/learning_rate/class tensorflow::Var does not exist.\n\t [[{{node training/SGD/Identity/ReadVariableOp}}]]"
     ]
    }
   ],
   "source": [
    "global sess\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "\n",
    "    # Train the head branches\n",
    "    # Passing layers=\"heads\" freezes all layers except the head\n",
    "    # layers. You can also pass a regular expression to select\n",
    "    # which layers to train by name pattern.\n",
    "    model.train(dataset_train, dataset_val, \n",
    "                learning_rate=config.LEARNING_RATE, \n",
    "                epochs=1, \n",
    "                layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
